{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "PySyft Keystone Secure DL CIFAR10 with Linear.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berenice2018/DeepLearning/blob/master/PySyft_Keystone_Secure_DL_CIFAR10_with_Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf9WwGD-uu5Q",
        "colab_type": "text"
      },
      "source": [
        "(notebook copy from Kaggle)\n",
        "\n",
        "**Issues with PySyft: (state of 2019, August 20th)** \n",
        "\n",
        "1.   Unfortunately, PySyft does not support CUDA. https://github.com/OpenMined/PySyft/issues/1893\n",
        "That is why I decided for a simple and small dataset. \n",
        "2.   https://github.com/OpenMined/PySyft/issues/2425\n",
        "3.https://github.com/OpenMined/PySyft/issues/2426 \n",
        "4. \"AttributeError: 'Linear' object has no attribute 'fix_prec'\"\n",
        "https://github.com/OpenMined/PySyft/pull/2364\n",
        "5. Running PySyft code often ends up with an empty AssertionError. It is related to the installation process, because `pip install` could not install succesfully. The current PySyft version does not fit with the current PyTorch, TorchVision versions. \n",
        "6. PySyft currenlty only supports exactly 2 workers, neither more no less. \n",
        "7. We get error messages, when we train with non-linear models, for example while using Convolutional and BatchNorm layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0pO5YODqCl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Data is shared in an encrypted way across multiple end devices via workers\n",
        "```(data.fix_precision().share(ada, bob, crypto_provider= crypto_provdr)```\n",
        "\n",
        "The end device downloads the current model, i.e. the model is shared with the end devices: \n",
        "``` model.fix_precision().share(ada,bob, crypto_provider=crypto_provdr)```\n",
        "\n",
        "\n",
        "The end device improves the model by learning from data on that end device, \n",
        "and then summarizes the changes as a small focused update. \n",
        "\n",
        "It is just this update to the model that is sent to the cloud (secure worker), using encryption, where it is averaged with other updates (coming from end devices) to improve the shared model. \n",
        "\n",
        "All the training data remains on the end devices. The model, the inputs, the model outputs, the weights, etc. will be encrypted as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2SWovhrtGBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "q1DbkeJZxdAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2bddc5f4-b803-4129-8862-183eda66f391"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import syft as sy\n",
        "\n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 12:56:11.471678 140100619605888 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0827 12:56:11.488156 140100619605888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dbYGqMYoxdAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize plot\n",
        "def plot_loss_acc(n_epochs, train_losses, valid_losses, valid_accuracies):\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(14,6), ncols=2)\n",
        "    ax1.plot(valid_losses, label='Validation loss')\n",
        "    ax1.plot(train_losses, label='Training loss')\n",
        "    ax1.legend(frameon=False)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    #x_ticks = [x for x in range(0,n_epochs,2)]\n",
        "    #plt.xticks(x_ticks)\n",
        "    \n",
        "    ax2.plot(valid_accuracies, label = 'Validation accuracy')\n",
        "    ax2.legend(frameon=False)\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    \n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "hxADDCgNxdAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create workers, \n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "ada = sy.VirtualWorker(hook, 'ada')\n",
        "bob = sy.VirtualWorker(hook, 'bob')\n",
        "#cyd = sy.VirtualWorker(hook, 'cyd')\n",
        "\n",
        "client = sy.VirtualWorker(hook, 'client')\n",
        "crypto_provdr = sy.VirtualWorker(hook, 'crypto_provdr') #gives crypto primitives we may need"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WXENc3QoxdAy",
        "colab_type": "code",
        "outputId": "f12b27be-259c-46a4-cc4b-8075f9c242d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5 ), (0.5,0.5,0.5 ))\n",
        "])\n",
        "\n",
        "# load the datasets\n",
        "fulltrainset = datasets.CIFAR10(root='./CIFAR10_data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10('~/.pytorch/CIFAR10_data', download=True, train=False, transform=transform)\n",
        "\n",
        "train_size = int(len(fulltrainset)* 0.8)\n",
        "valid_size = len(fulltrainset) - train_size\n",
        "\n",
        "# split the dataset\n",
        "trainset, validationset = torch.utils.data.random_split(fulltrainset, [train_size, valid_size])\n",
        "trainset = trainset.dataset\n",
        "validationset = validationset.dataset\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR10_data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:04, 40791182.82it/s]                               \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.pytorch/CIFAR10_data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:04, 41399959.28it/s]                               \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "azt73D0ixdA0",
        "colab_type": "code",
        "outputId": "a47f1820-8fd9-4059-e067-3bf8ac6c0770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# we assume that the server has access to some data to first train its model\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# The client has some data and gets predictions on it using the server's model. \n",
        "# The client encrypts its data by sharing it additively across the workers ada, bob, cyd.\n",
        "valid_loader = torch.utils.data.DataLoader(validationset, batch_size=64, shuffle=True)\n",
        "\n",
        "private_valid_loader = []\n",
        "for data, target in valid_loader:\n",
        "    private_valid_loader.append(\n",
        "        (data.fix_precision().share(ada, bob, crypto_provdr),\n",
        "        target.fix_precision().share(ada, bob, crypto_provdr)\n",
        "        ))\n",
        "\n",
        "    \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=64, shuffle=True)\n",
        "\n",
        "len_trainloader = len(train_loader)\n",
        "len_validloader = len(valid_loader)\n",
        "print(len_trainloader)\n",
        "\n",
        "# Let's check that our trainloader returns a pointer to a batch, and that transformations are applied\n",
        "#data, labels = next(iter(train_loader))\n",
        "#data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63epObo5xdA2",
        "colab_type": "text"
      },
      "source": [
        "**Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ll1P35xoxdA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    \n",
        "    # (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "class MyCifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCifarNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('x {}, size {}'.format(x.shape, x.size(0))) # x torch.Size([64, 1, 28, 28]), size 0\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_P1_Ma_xdA4",
        "colab_type": "text"
      },
      "source": [
        "**Functions train, test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jelf7lf7xdA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    '''Takes in a module and initializes all linear layers with weight\n",
        "       values taken from a normal distribution.'''\n",
        "    classname = m.__class__.__name__\n",
        "    # for every Linear layer in a model\n",
        "    if classname.find('Linear') != -1:\n",
        "        n = m.in_features\n",
        "        # m.weight.data shoud be taken from a normal distribution\n",
        "        m.weight.data.normal_(0, 1/np.sqrt(n))\n",
        "        # m.bias.data should be 0\n",
        "        m.bias.data.fill_(0)\n",
        "            \n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    # initialize variables to monitor training and validation loss\n",
        "    train_loss = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader): # <-- now it is a distributed dataset\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(' \\tTrain. Loss: {:.6f}'.format(current_loss))    \n",
        "\n",
        "        # get the loss per batch and accumulate\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    # calculate the average loss per epoch\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    #print('Epoch: {} \\tTrain. Loss: {:.6f}'.format(epoch, train_loss))    \n",
        "    return train_loss\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dznlwy9-xdA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct/len(test_loader.dataset)\n",
        "    #print('Epoch: {} \\tValid. Loss: {:.6f}'.format(epoch, test_loss))    \n",
        "    return test_loss, accuracy\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ULhlb5y8xdA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 64\n",
        "        self.epochs = 5\n",
        "        self.lr = 0.001\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU4-pNJOxdA-",
        "colab_type": "text"
      },
      "source": [
        "**Start the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bYWWeC3mxdA_",
        "colab_type": "code",
        "outputId": "79c1bd1c-67bd-4ff0-9b93-76ef23055666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "model = Model() # MyCifarNet()\n",
        "model.apply(weights_init_normal)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [5, 10, 14], 0.1)\n",
        "\n",
        "valid_losses = []\n",
        "train_losses = []\n",
        "valid_accuracies = []\n",
        "valid_loss_min = np.Inf\n",
        "    \n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    \n",
        "    # initialize variables to monitor training and validation loss\n",
        "    training_loss = 0.0\n",
        "    training_accuracy = 0.0\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "        \n",
        "    training_loss =  train(args, model, device, train_loader, optimizer, epoch)\n",
        "    validation_loss, validation_accuracy = validate(args, model, device, test_loader)\n",
        "\n",
        "    #if scheduler is not None:\n",
        "      #scheduler.step(validation_loss) # in case of ReduceOnPlateau \n",
        "\n",
        "    ###### print training/validation statistics \n",
        "    train_losses.append(training_loss)\n",
        "    valid_losses.append(validation_loss)\n",
        "    valid_accuracies.append(validation_accuracy)\n",
        "\n",
        "    #hour, minute, second = get_time()\n",
        "    print('Epoch: {} \\tTrain. Loss: {:.6f} \\tValid. Loss: {:.6f} \\t Accur.: {:.6f}'.format(\n",
        "              epoch,\n",
        "              training_loss,\n",
        "              validation_loss,\n",
        "              validation_accuracy ))\n",
        "\n",
        "    ###### TODO: save the model if validation loss has decreased\n",
        "    if validation_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased by {:.6f}'.format(validation_loss - valid_loss_min))\n",
        "        #torch.save(model.state_dict(), save_path)\n",
        "        valid_loss_min = validation_loss\n",
        "    \n",
        "##### visualize\n",
        "plot_loss_acc(args.epochs, train_losses, valid_losses, valid_accuracies)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTrain. Loss: 2.079996 \tValid. Loss: 1.957968 \t Accur.: 31.740000\n",
            "Validation loss decreased by -inf\n",
            "Epoch: 2 \tTrain. Loss: 1.902019 \tValid. Loss: 1.855560 \t Accur.: 35.660000\n",
            "Validation loss decreased by -0.102408\n",
            "Epoch: 3 \tTrain. Loss: 1.823125 \tValid. Loss: 1.796728 \t Accur.: 38.130000\n",
            "Validation loss decreased by -0.058832\n",
            "Epoch: 4 \tTrain. Loss: 1.773130 \tValid. Loss: 1.755399 \t Accur.: 39.290000\n",
            "Validation loss decreased by -0.041329\n",
            "Epoch: 5 \tTrain. Loss: 1.749216 \tValid. Loss: 1.751922 \t Accur.: 39.330000\n",
            "Validation loss decreased by -0.003476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAGoCAYAAADVZM+hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WA-PNGpGxdBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c46eaaa3-0f48-48e2-e94f-ee4e7970955a"
      },
      "source": [
        "# As the server, send the model to the workers holding the data. The weights are not disclosed\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "    \n",
        "model.fix_precision().share(ada, bob, crypto_provider=crypto_provdr)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=3072, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iB3_pEUMxdBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform the encrypted evaluation. The model weights, the data inputs, the prediction and \n",
        "# the target used for scoring are encrypted!\n",
        "\n",
        "def validate_encrypted(args, model, valid_loader):\n",
        "    model.eval()\n",
        "    n_correct_private = 0\n",
        "    n_total = 0\n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in valid_loader:\n",
        "            output = model(data)\n",
        "            prediction = output.argmax(1, keepdim=True)\n",
        "            n_correct_private += prediction.eq(target.view_as(prediction)).sum()\n",
        "            n_total += args.test_batch_size\n",
        "            \n",
        "            #decrypt from the server side the final score of the batch items, \n",
        "            # to verify if the predictions were on average good\n",
        "            n_correct = n_correct_private.copy().get().float_precision().long().item()\n",
        "            print('Valid. set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "                n_correct, n_total,\n",
        "                100. * n_correct / n_total))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pf00j5tPxdBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "d1524c36-4605-4d6f-8986-5e2c5a3bd609"
      },
      "source": [
        "validate_encrypted(args, model, private_valid_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3227373c4454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_encrypted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_valid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c5aa80639eff>\u001b[0m in \u001b[0;36mvalidate_encrypted\u001b[0;34m(args, model, valid_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mn_correct_private\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c30436bf3dee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mhandle_func_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mnew_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;31m# Put back the wrappers where needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             response = syft.frameworks.torch.hook_args.hook_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Try to get recursively the attributes in cmd = \"<attr1>.<attr2>.<attr3>...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0mUn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhook\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mits\u001b[0m \u001b[0mdetailed\u001b[0m \u001b[0mbehaviour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mhandle_func_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Try to get recursively the attributes in cmd = \"<attr1>.<attr2>.<attr3>...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(bias, input_tensor, weight)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mmatmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matmul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_public_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matmul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_private_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matmul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py\u001b[0m in \u001b[0;36m_private_mul\u001b[0;34m(self, other, equation)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdditiveSharingTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrypto_provider\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cfLBVlAoxdBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(f'objects of ada= {len(ada._objects)}, bob= {len(bob._objects)}, cyd= {len(cyd._objects)}')\n",
        "\n",
        "#ada.clear_objects()\n",
        "#bob.clear_objects()\n",
        "#cyd.clear_objects()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}