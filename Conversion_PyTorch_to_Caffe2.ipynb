{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conversion PyTorch to Caffe2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "XGBKrAJcEIvB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berenice2018/DeepLearning/blob/master/Conversion_PyTorch_to_Caffe2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBKrAJcEIvB",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTU665fhTcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBsJZ3vhI51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install onnx\n",
        "\n",
        "help(torch.onnx.export)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgw2hbFPhhNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade --force-reinstall torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zme3X9MtCW0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torchvision\n",
        "\n",
        "#!pip uninstall torchvision\n",
        "#!pip install -c pytorch pytorch-nightly torchvision cudatoolkit=10.0\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJQHdjih4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "import torchvision\n",
        "\n",
        "# Super Resolution model definition in PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoyz_BGNEGEo",
        "colab_type": "text"
      },
      "source": [
        "### Define the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv5234YBEFTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/Fer-dataset/' \n",
        "checkpoint_name = 'akash-mobilenet_v2-FER1-60perc.pt'\n",
        "onnx_export_path = base_path + \"ONNX/akash_mobilenet_60_bc2.onnx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6Ct8SsD-Fg",
        "colab_type": "text"
      },
      "source": [
        "To export a model, you call the torch.onnx._export() function. \n",
        "This will execute the model, recording a trace of what operators are used to compute the outputs. \n",
        "Because _export runs the model, we need provide an input tensor x. \n",
        "The values in this tensor are not important; it can be an image or a \n",
        "random tensor as long as it is the right size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7EoGmah_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Standard ImageNet input - 3 channels, 224x224,\n",
        "# values don't matter as we care about network structure.\n",
        "# But they can also be real inputs.\n",
        "\n",
        "\n",
        "# A model class instance (class not shown)\n",
        "torch_model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "torch_model.classifier[1] = torch.nn.Linear(1280, 7)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Initialize model with the pretrained weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "\n",
        "# Load the weights from a file (.pth usually)\n",
        "state_dict = torch.load(base_path + checkpoint_name, map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the weights now into a model net architecture defined by our class\n",
        "torch_model.load_state_dict(state_dict)\n",
        "\n",
        "# set the train mode to false since we will only run the forward pass.\n",
        "torch_model.train(False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO9kXe5iQ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the right input shape (e.g. for an image)\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "\n",
        "torch.onnx.export(torch_model, dummy_input, onnx_export_path)\n",
        "\n",
        "# Export the model\n",
        "torch_out = torch.onnx._export(torch_model,             # model being run\n",
        "                               dummy_input,             # model input (or a tuple for multiple inputs)\n",
        "                               export_path,             # where to save the model (can be a file or file-like object)\n",
        "                               export_params=True)      # store the trained parameter weights inside the model file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxPrnCMEcvG",
        "colab_type": "text"
      },
      "source": [
        "torch_out is the output after executing the model. Normally you can ignore this output, but here we will use it to verify that the model we exported computes the same values when run in Caffe2.\n",
        "\n",
        "Now letâ€™s take the ONNX representation and use it in Caffe2. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that Caffe2 and PyTorch are computing the same value for the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG8f0a7YEdtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import onnx\n",
        "import caffe2.python.onnx.backend as onnx_caffe2_backend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8VKht6mEnqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bada61c-739f-46ab-8a23-a0083fe12acf"
      },
      "source": [
        "# Load the ONNX ModelProto object. model is a standard Python protobuf object\n",
        "model = onnx.load(export_path)\n",
        "\n",
        "# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n",
        "# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n",
        "# availiable soon.\n",
        "prepared_backend = onnx_caffe2_backend.prepare(model)\n",
        "\n",
        "# run the model in Caffe2\n",
        "\n",
        "# Construct a map from input names to Tensor data.\n",
        "# The graph of the model itself contains inputs for all weight parameters, after the input image.\n",
        "# Since the weights are already embedded, we just need to pass the input image.\n",
        "# Set the first input.\n",
        "W = {model.graph.input[0].name: dummy_input.data.numpy()}\n",
        "\n",
        "# Run the Caffe2 net:\n",
        "c2_out = prepared_backend.run(W)[0]\n",
        "\n",
        "# Verify the numerical correctness upto 3 decimal places\n",
        "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n",
        "\n",
        "print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exported model has been executed on Caffe2 backend, and the result looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}