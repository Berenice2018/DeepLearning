{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conversion PyTorch to Caffe2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "XGBKrAJcEIvB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berenice2018/DeepLearning/blob/master/Conversion_PyTorch_to_Caffe2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WlAXUalbmW2",
        "colab_type": "text"
      },
      "source": [
        "https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html\n",
        "\n",
        "https://blog.exxactcorp.com/pytorch-release-v1-2-0-new-torchscript-api-with-improved-python-language-coverage-expanded-onnx-export-nn-transformer/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBKrAJcEIvB",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTU665fhTcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBsJZ3vhI51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install onnx\n",
        "\n",
        "help(torch.onnx.export)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgw2hbFPhhNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade --force-reinstall torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zme3X9MtCW0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall torchvision\n",
        "#!pip install -c pytorch pytorch-nightly torchvision cudatoolkit=10.0\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJQHdjih4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "import torchvision\n",
        "\n",
        "# Super Resolution model definition in PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fTGvFLCUtxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c117d0f6-c02f-4292-b0e7-13405daacd56"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n",
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoyz_BGNEGEo",
        "colab_type": "text"
      },
      "source": [
        "### Define the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv5234YBEFTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/Fer-dataset/' \n",
        "checkpoint_name = 'akash-mobilenet_v2-FER1-60perc.pt'\n",
        "onnx_export_path = base_path + \"ONNX/akash_mobilenet_60_bc2.onnx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6Ct8SsD-Fg",
        "colab_type": "text"
      },
      "source": [
        "To export a model, you call the torch.onnx._export() function. \n",
        "This will execute the model, recording a trace of what operators are used to compute the outputs. \n",
        "Because _export runs the model, we need provide an input tensor x. \n",
        "The values in this tensor are not important; it can be an image or a \n",
        "random tensor as long as it is the right size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7EoGmah_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Standard ImageNet input - 3 channels, 224x224,\n",
        "# values don't matter as we care about network structure.\n",
        "# But they can also be real inputs.\n",
        "\n",
        "\n",
        "# A model class instance (class not shown)\n",
        "torch_model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "torch_model.classifier[1] = torch.nn.Linear(1280, 7)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Initialize model with the pretrained weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "\n",
        "# Load the weights from a file (.pth usually)\n",
        "state_dict = torch.load(base_path + checkpoint_name, map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the weights now into a model net architecture defined by our class\n",
        "torch_model.load_state_dict(state_dict)\n",
        "\n",
        "# set the train mode to false since we will only run the forward pass.\n",
        "torch_model.train(False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO9kXe5iQ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the right input shape (e.g. for an image)\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "\n",
        "torch.onnx.export(torch_model, dummy_input, onnx_export_path)\n",
        "\n",
        "# Export the model\n",
        "torch_out = torch.onnx._export(torch_model,             # model being run\n",
        "                               dummy_input,             # model input (or a tuple for multiple inputs)\n",
        "                               onnx_export_path,             # where to save the model (can be a file or file-like object)\n",
        "                               export_params=True)      # store the trained parameter weights inside the model file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxPrnCMEcvG",
        "colab_type": "text"
      },
      "source": [
        "torch_out is the output after executing the model. Normally you can ignore this output, but here we will use it to verify that the model we exported computes the same values when run in Caffe2.\n",
        "\n",
        "Now let’s take the ONNX representation and use it in Caffe2. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that Caffe2 and PyTorch are computing the same value for the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG8f0a7YEdtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import onnx\n",
        "import caffe2.python.onnx.backend as onnx_caffe2_backend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8VKht6mEnqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "a19de607-ee4b-46df-e7c3-783f2a69f33c"
      },
      "source": [
        "# Load the ONNX ModelProto object. model is a standard Python protobuf object\n",
        "model = onnx.load(onnx_export_path)\n",
        "\n",
        "# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n",
        "# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n",
        "# availiable soon.\n",
        "prepared_backend = onnx_caffe2_backend.prepare(model)\n",
        "\n",
        "# run the model in Caffe2\n",
        "\n",
        "# Construct a map from input names to Tensor data.\n",
        "# The graph of the model itself contains inputs for all weight parameters, after the input image.\n",
        "# Since the weights are already embedded, we just need to pass the input image.\n",
        "# Set the first input.\n",
        "W = {model.graph.input[0].name: dummy_input.data.numpy()}\n",
        "\n",
        "print(model.graph.input[0])\n",
        "print(model.graph.output[0])\n",
        "\n",
        "# Run the Caffe2 net:\n",
        "c2_out = prepared_backend.run(W)[0]\n",
        "\n",
        "# Verify the numerical correctness upto 3 decimal places\n",
        "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n",
        "\n",
        "print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: \"input.1\"\n",
            "type {\n",
            "  tensor_type {\n",
            "    elem_type: 1\n",
            "    shape {\n",
            "      dim {\n",
            "        dim_value: 1\n",
            "      }\n",
            "      dim {\n",
            "        dim_value: 3\n",
            "      }\n",
            "      dim {\n",
            "        dim_value: 256\n",
            "      }\n",
            "      dim {\n",
            "        dim_value: 256\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "name: \"465\"\n",
            "type {\n",
            "  tensor_type {\n",
            "    elem_type: 1\n",
            "    shape {\n",
            "      dim {\n",
            "        dim_value: 1\n",
            "      }\n",
            "      dim {\n",
            "        dim_value: 7\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Exported model has been executed on Caffe2 backend, and the result looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5u_Vit7HH3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "564aa575-7189-4225-dab2-5c4cb16ff56a"
      },
      "source": [
        "prepared_backend # caffe2.python.onnx.backend_rep.Caffe2Rep"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<caffe2.python.onnx.backend_rep.Caffe2Rep at 0x7fa3d2399048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUhxxlvuHaqW",
        "colab_type": "text"
      },
      "source": [
        "Running the model on mobile devices\n",
        "So far we have exported a model from PyTorch and shown how to load it and run it in Caffe2. Now that the model is loaded in Caffe2, we can convert it into a format suitable for running on mobile devices.\n",
        "\n",
        "We will use Caffe2’s mobile_exporter to generate the two model protobufs that can run on mobile. The first is used to initialize the network with the correct weights, and the second actual runs executes the model. We will continue to use the small super-resolution model for the rest of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX5ijZkdHbxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the workspace and the model proto from the internal representation\n",
        "c2_workspace = prepared_backend.workspace\n",
        "c2_model = prepared_backend.predict_net\n",
        "\n",
        "# Now import the caffe2 mobile exporter\n",
        "from caffe2.python.predictor import mobile_exporter\n",
        "\n",
        "# call the Export to get the predict_net, init_net. These nets are needed for running things on mobile\n",
        "init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)\n",
        "\n",
        "# Let's also save the init_net and predict_net to a file that we will later use for running them on mobile\n",
        "with open('init_net.pb', \"wb\") as fopen:\n",
        "    fopen.write(init_net.SerializeToString())\n",
        "with open('predict_net.pb', \"wb\") as fopen:\n",
        "    fopen.write(predict_net.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12trM1rXIADT",
        "colab_type": "text"
      },
      "source": [
        "init_net has the model parameters and the model input embedded in it and predict_net will be used to guide the init_net execution at run-time. In this tutorial, we will use the init_net and predict_net generated above and run them in both normal Caffe2 backend and mobile and verify that the output high-resolution cat image produced in both runs is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-BTrfphIBIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "from caffe2.proto import caffe2_pb2\n",
        "from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot\n",
        "from skimage import io, transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gRNJmR_IO6Y",
        "colab_type": "text"
      },
      "source": [
        "load the image, pre-process it using standard skimage python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yUYygqTIOFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "063652af-e4c7-4a5d-e3d6-a68c4cd30f6d"
      },
      "source": [
        "# path to test image\n",
        "path_to_Test_input_img = base_path + \"testface-happy-sw.jpg\"\n",
        "path_to_Test_output_img = base_path + \"testface_224x224.jpg\"\n",
        "\n",
        "\n",
        "# load the image\n",
        "img_in = io.imread(path_to_Test_input_img)\n",
        "\n",
        "# resize the image to dimensions 224x224\n",
        "img = transform.resize(img_in, [256, 256])\n",
        "\n",
        "\n",
        "# save this resized image to be used as input to the model\n",
        "io.imsave(path_to_Test_output_img, img)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEYdOOFOJdqb",
        "colab_type": "text"
      },
      "source": [
        "Now, as a next step, let’s take the resized  image and run the  model in Caffe2 backend and save the output image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Kv6RNIJcFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1c521f47-67cc-462f-ba36-84b6a4577958"
      },
      "source": [
        "# load the resized image and convert it to Ybr format\n",
        "img = Image.open(path_to_Test_output_img)\n",
        "#img_ycbcr = img.convert('YCbCr')\n",
        "#img_y, img_cb, img_cr = img_ycbcr.split()\n",
        "\n",
        "print(img.size)\n",
        "\n",
        "# Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized\n",
        "workspace.RunNetOnce(init_net)\n",
        "workspace.RunNetOnce(predict_net)\n",
        "\n",
        "# Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify\n",
        "# what our input and output blob names are.\n",
        "\n",
        "#print(net_printer.to_string(predict_net))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2TEFtYZKPhy",
        "colab_type": "text"
      },
      "source": [
        "From the above output, we can see that input is named “input.1” and output is named “465”(it is a little bit weird that we will have numbers as blob names but this is because the tracing JIT produces numbered entries for the models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQRaq4BWKflV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dd4875d-5da9-4081-f31b-d6231b35162c"
      },
      "source": [
        "# Now, let's also pass in the resized cat image for processing by the model.\n",
        "#workspace.FeedBlob(\"input.1\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n",
        "\n",
        "pass_img = np.broadcast_to(img, (1, 3, 256, 256)) \n",
        "print (pass_img.shape)\n",
        "\n",
        "workspace.FeedBlob(\"input.1\", np.array(pass_img)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n",
        "\n",
        "# run the predict_net to get the model output\n",
        "workspace.RunNetOnce(predict_net)\n",
        "\n",
        "# Now let's get the model output blob\n",
        "img_out = workspace.FetchBlob(\"465\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:caffe2.python.workspace:Original python traceback for operator `0` in network `torch-jit-export_predict` in exception above (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 256, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-e7be25053d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# run the predict_net to get the model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunNetOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Now let's get the model output blob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mRunNetOnce\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWorkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_failed_op_net_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mGetNetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mStringifyProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mCallWithExceptionIntercept\u001b[0;34m(func, op_id_fetcher, net_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCallWithExceptionIntercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mop_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at conv_op_impl.h:28] X.dim() == filter.dim(). 6 vs 4\nError from operator: \ninput: \"input.1\" input: \"1\" output: \"315\" name: \"\" type: \"Conv\" arg { name: \"strides\" ints: 2 ints: 2 } arg { name: \"pads\" ints: 1 ints: 1 ints: 1 ints: 1 } arg { name: \"dilations\" ints: 1 ints: 1 } arg { name: \"kernels\" ints: 3 ints: 3 } arg { name: \"group\" i: 1 } device_option { device_type: 0 device_id: 0 }frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7fa3e6a69e17 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x2f9887f (0x7fa3e9e5287f in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)\nframe #2: <unknown function> + 0x2dd7c8d (0x7fa3e9c91c8d in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x2cc29fd (0x7fa3e9b7c9fd in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)\nframe #4: caffe2::SimpleNet::Run() + 0x167 (0x7fa3e999a657 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)\nframe #5: caffe2::Workspace::RunNetOnce(caffe2::NetDef const&) + 0x2b (0x7fa3e99e151b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)\nframe #6: <unknown function> + 0x55c84 (0x7fa3e2ed9c84 in /usr/local/lib/python3.6/dist-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x955e1 (0x7fa3e2f195e1 in /usr/local/lib/python3.6/dist-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\nframe #8: PyCFunction_Call + 0x103 (0x562433 in /usr/bin/python3)\nframe #9: _PyEval_EvalFrameDefault + 0x58c6 (0x4fed26 in /usr/bin/python3)\nframe #10: /usr/bin/python3() [0x4f6128]\nframe #11: /usr/bin/python3() [0x4f7d60]\nframe #12: /usr/bin/python3() [0x4f876d]\nframe #13: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #14: /usr/bin/python3() [0x4f7a28]\nframe #15: /usr/bin/python3() [0x4f876d]\nframe #16: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #17: /usr/bin/python3() [0x4f6128]\nframe #18: /usr/bin/python3() [0x517a9a]\nframe #19: /usr/bin/python3() [0x4f858d]\nframe #20: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #21: /usr/bin/python3() [0x4f6128]\nframe #22: /usr/bin/python3() [0x4f7d60]\nframe #23: /usr/bin/python3() [0x4f876d]\nframe #24: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #25: /usr/bin/python3() [0x4f6128]\nframe #26: /usr/bin/python3() [0x4f7d60]\nframe #27: /usr/bin/python3() [0x4f876d]\nframe #28: _PyEval_EvalFrameDefault + 0x1260 (0x4fa6c0 in /usr/bin/python3)\nframe #29: /usr/bin/python3() [0x4f6128]\nframe #30: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\nframe #31: /usr/bin/python3() [0x5a1481]\nframe #32: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\nframe #33: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\nframe #34: /usr/bin/python3() [0x4f6128]\nframe #35: /usr/bin/python3() [0x4f7d60]\nframe #36: /usr/bin/python3() [0x4f876d]\nframe #37: _PyEval_EvalFrameDefault + 0x1260 (0x4fa6c0 in /usr/bin/python3)\nframe #38: /usr/bin/python3() [0x4f6128]\nframe #39: /usr/bin/python3() [0x4f7d60]\nframe #40: /usr/bin/python3() [0x4f876d]\nframe #41: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #42: /usr/bin/python3() [0x4f7a28]\nframe #43: /usr/bin/python3() [0x4f876d]\nframe #44: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #45: /usr/bin/python3() [0x4f7a28]\nframe #46: /usr/bin/python3() [0x4f876d]\nframe #47: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #48: /usr/bin/python3() [0x4f6128]\nframe #49: /usr/bin/python3() [0x56fe24]\nframe #50: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\nframe #51: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\nframe #52: /usr/bin/python3() [0x4f6128]\nframe #53: /usr/bin/python3() [0x56fe24]\nframe #54: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\nframe #56: /usr/bin/python3() [0x4f6128]\nframe #57: /usr/bin/python3() [0x4f7d60]\nframe #58: /usr/bin/python3() [0x4f876d]\nframe #59: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #60: /usr/bin/python3() [0x4f7a28]\nframe #61: /usr/bin/python3() [0x4f876d]\nframe #62: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\nframe #63: _PyFunction_FastCallDict + 0xf5 (0x4f4065 in /usr/bin/python3)\n"
          ]
        }
      ]
    }
  ]
}