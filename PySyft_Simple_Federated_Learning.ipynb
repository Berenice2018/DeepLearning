{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySyft Simple Federated Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berenice2018/DeepLearning/blob/master/PySyft_Simple_Federated_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AdE3SBhPLvM",
        "colab_type": "text"
      },
      "source": [
        "### Imports, setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKqONFBkLobW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYwkx91pLr_h",
        "colab_type": "code",
        "outputId": "28d01ef4-a5e1-4496-d0e4-c30d1116413c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import syft as sy\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 19:15:48.256587 140064218503040 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0722 19:15:48.277531 140064218503040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1-OjJVKL0TC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a96a42d7-7d6e-436d-cf6b-aeafcaa27464"
      },
      "source": [
        "# save the model on Google Drive, link Google drive to this notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# After executing this cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/gdrive/My Drive/Colab Notebooks/flower_data/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "picco_test  test  train  train_ofgdrive  valid\tvalid_ofgdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FfPa7ijL94N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths to training and test data\n",
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/flower_data/'\n",
        "train_dir = data_dir + 'train'\n",
        "valid_dir = data_dir + 'valid'\n",
        "\n",
        "#os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/\")\n",
        "test_dir = data_dir + 'test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qj0XFdPVaP",
        "colab_type": "text"
      },
      "source": [
        "### Architecture and helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42QrxlZWMFpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make data loader based on the selected pre-trained model\n",
        "def create_loaders(base, final = False):\n",
        "    print('returning datasets')\n",
        "    # ResNet, DenseNet expect 224, Inception expects 299\n",
        "    img_size = 299 if base == 'Inception' else 224 \n",
        "\n",
        "    transforms_train = transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    transforms_test = transforms.Compose([\n",
        "        transforms.Resize(img_size + 1),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    # Load the datasets with ImageFolder\n",
        "    trainset = datasets.ImageFolder(train_dir, transform=transforms_train)\n",
        "    validationset = datasets.ImageFolder(valid_dir, transform=transforms_train)\n",
        "    testset = datasets.ImageFolder(valid_dir, transform=transforms_test)\n",
        "       \n",
        "    return trainset, validationset, testset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pimMQCE-LfV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transforms a torch.Dataset or a sy.BaseDataset into a sy.FederatedDataset. \n",
        "def dataset_federate(dataset, workers):\n",
        "    print('dataset_federate')\n",
        "\n",
        "    datasets = []\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "    \n",
        "    for dataset_idx, (data, targets) in enumerate(data_loader):\n",
        "        worker = workers[dataset_idx % len(workers)]\n",
        "        data = data.send(worker)\n",
        "        targets = targets.send(worker)\n",
        "        datasets.append(sy.BaseDataset(data, targets))\n",
        "    \n",
        "    fed_dataset = sy.FederatedDataset(datasets)\n",
        "    fed_loader = sy.FederatedDataLoader(fed_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
        "    \n",
        "    return fed_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM9tZW_rMg3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassifierH1(nn.Module):\n",
        "    def __init__(self, inp = 784, h1=512, out = 10, d=0.3):\n",
        "        super(ClassifierH1, self).__init__()\n",
        "        self.fc1 = nn.Linear(inp, h1)\n",
        "        self.fc2 = nn.Linear(h1, out)\n",
        "        \n",
        "        self.dropout = nn.Dropout(d)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)        \n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a7JJyHdMwad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions to have flexibility with choice if needed\n",
        "# The optimizer to be used for training\n",
        "def create_optimizer(params, opt, lr):\n",
        "    return optim.SGD(params, lr= lr)\n",
        "\n",
        "# Scheduler that automatically adjusts learning rate of optimizer\n",
        "def create_scheduler(opt, p=7, f = 0.1):\n",
        "    return optim.lr_scheduler.ReduceLROnPlateau(opt, patience = p, factor=f)\n",
        "\n",
        "# Loss function selector\n",
        "def create_loss(ls):\n",
        "    return nn.CrossEntropyLoss() if ls == 'Entr' else nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKlGNScGM00k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNetwork():\n",
        "       \n",
        "    def __init__(self, base, clf, kwargs):\n",
        "        # Additional variables will be used to track training progress for easier re-loads, plotting, etc.\n",
        "        self.last_epoch = 0       # Keeps track of number of epochs trained\n",
        "        self.min_vloss = np.inf   # Keeps track of lowest validation loss\n",
        "        self.lr_hist = []         # Tracks learn rate changes over epochs \n",
        "        self.vloss_hist = []      # Tracks validation loss changes over epochs\n",
        "        self.tloss_hist = []      # Tracks training loss changes over epochs\n",
        "        self.acc_hist = []        # Tracks accuracy rate changes over epochs\n",
        "        self.opt_dict = None      # Stores optimizer dictionary\n",
        "        self.sch_dict = None      # Stores scheduler dictionary\n",
        "        self.sch_wait = None      # Stores scheduler \"patience\" parameter\n",
        "        self.clf_name = clf       # Stores the name of the chosen classifier\n",
        "        self.base_name = base     # Stores the name of chosen CNN\n",
        "        self.loss_name = None     # Stores the name of chosen loss function\n",
        "        self.opt_name = None      # Stores the name of chosen optimizer method\n",
        "        self.layer_dict = kwargs  # Stores the info on number of nodes in each clf layer, as well as dropout %\n",
        "        # When the datasets are created, each class value automatically got assigned an index.\n",
        "        # These will be created and stored in the training function \n",
        "        self.class_to_idx = None\n",
        "        self.idx_to_class = None\n",
        "        \n",
        "        # Create vanilla feature net and classifier\n",
        "        base_net = self.import_model(base)\n",
        "        my_clf = self.create_clf(base, clf, kwargs)\n",
        "\n",
        "        # Adding the classifier to pre-trained network (replacing last layer)\n",
        "        if base == 'ResNet152':\n",
        "            base_net.fc = my_clf\n",
        "        elif base == 'Inception':\n",
        "            base_net.fc = my_clf\n",
        "        elif base == 'DenseNet161':\n",
        "            base_net.classifier = my_clf\n",
        "            \n",
        "        self.my_net = base_net \n",
        "        self.my_clf = my_clf\n",
        "        \n",
        "        print('Model created from base_net {}, Clf {} with {}'.format(base, clf, kwargs))\n",
        "        \n",
        "    # Pre-trained conv-net used for feature extraction before final classification\n",
        "    def import_model(self, mod, tr = True):\n",
        "        if mod == 'ResNet152':\n",
        "            m = models.resnet152(pretrained=tr)\n",
        "        elif mod == 'Inception':\n",
        "            m = models.inception_v3(pretrained=tr) # aux_logits=False)\n",
        "        elif mod == 'DenseNet161':\n",
        "            m = models.densenet161(pretrained=tr)\n",
        "        else:\n",
        "            print('No base model imported!')\n",
        "        \n",
        "        # Keep the weights fixed on the pre-trained model?\n",
        "        if tr:\n",
        "            for param in m.parameters():\n",
        "                param.requires_grad = True \n",
        "        \n",
        "        return m     \n",
        "        \n",
        "    # Create a specific classifier\n",
        "    def create_clf(self, base, clf, kwargs):\n",
        "        # Define input size, based on the output of pre-trained models:\n",
        "        if base == 'ResNet152':\n",
        "            in_size = 2048\n",
        "        elif base == 'Inception':\n",
        "            in_size = 2048\n",
        "        elif base == 'DenseNet161':\n",
        "            in_size = 2208\n",
        "        if clf == 'H1':\n",
        "            return ClassifierH1(in_size, kwargs['h1'], kwargs['out'], kwargs['d'])\n",
        "        else:\n",
        "            print('Classifier not found')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYMgdqwWNJ91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, n_epochs, isFinal, train_params = None):\n",
        "    criterion = create_loss(train_params['loss'])\n",
        "    optimizer = create_optimizer(model.my_clf.parameters(), train_params['opt'], train_params['lr'])\n",
        "    scheduler = create_scheduler(optimizer, train_params['wait'])\n",
        "    model.loss_name = train_params['loss']\n",
        "    model.opt_name = train_params['opt']\n",
        "    model.sch_wait = train_params['wait']\n",
        "    model.vloss_min = np.inf\n",
        "  \n",
        "  # check if CUDA is available\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    if not train_on_gpu:\n",
        "        print('CUDA is not available.  Training on CPU ...')\n",
        "        device = \"cpu\"\n",
        "    else:\n",
        "        print('CUDA is available!  Training on GPU ...')\n",
        "        device = \"cuda\"\n",
        "        \n",
        "    model.my_net.to(device)\n",
        "    \n",
        "    print('*** Training starting ***')\n",
        "   \n",
        "    \n",
        "    for epoch in range(2):\n",
        "      \n",
        "      # keep track of training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        model.my_net.train()\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # PySyft: send the model to the right location\n",
        "            print('model sent to data.location {}'.format(data.location))\n",
        "            model.my_net.send(data.location)\n",
        "\n",
        "            \n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.my_net(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # PySyft: get the smarter model back\n",
        "            model.my_net.get()\n",
        "            \n",
        "            # update training loss\n",
        "            syft_loss = loss.get() # PySyft: get the loss back\n",
        "            train_loss += syft_loss.item() * data.size(0)\n",
        "        \n",
        "        print('   Train loss: \\t{:.6f}'.format(train_loss))\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGXMjxfOOJ60",
        "colab_type": "text"
      },
      "source": [
        "### Instantiation,  hyperparams, model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkAoN4RQNXLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File name for saving model\n",
        "filename_drive_best = './densenet161_test.pt'\n",
        "filename_drive_fin = './densenet161_test.pt'\n",
        "\n",
        "# Defining network model \n",
        "# Base net can be 'Inception', 'ResNet152' or 'DenseNet161'\n",
        "\n",
        "base_net = 'DenseNet161'   \n",
        "clf = 'H1' # H1 has one hidden layer, H2 has two\n",
        "layers = {'out':102, 'd':0.2, 'h1':1024, 'h2':512} # Input size is pre-determined and set based on chosen pre-trained model. d is for droput rate.\n",
        "\n",
        "# Training parameters\n",
        "epochs = 2\n",
        "\n",
        "# If set to False, 20% of train set is reserved for validation, \n",
        "# and test set reserved for testing the trained model\n",
        "isFinal = True \n",
        "train_params = {\n",
        "                'opt':   'SGD',   # Optimizer to be used for training\n",
        "                'loss':  '',  # or 'Entr' for CrossEntropyLoss. Loss function to be used for training\n",
        "                'lr':    1e-3,    # Learning rate for training\n",
        "                'wait':  7        # \"patience\" parameter for scheduler function from Pytorch.\n",
        "                }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25JyF08Ng0t",
        "colab_type": "code",
        "outputId": "1a07a291-6d38-4510-fdca-dbc9d6fee735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Create a blank model or load an existing one\n",
        "model = MyNetwork(base_net, clf, layers)\n",
        "\n",
        "\n",
        "def unfreeze(model):\n",
        "    for name, child in model.named_children():\n",
        "        if name in ['denseblock4', 'denseblock3', 'denseblock2', 'denseblock1']: #, 'denseblock1']:\n",
        "            print(name + ' is unfrozen')\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            print(name + ' is unfrozen')\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "unfreeze(model.my_net.features)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/checkpoints/densenet161-8d451a50.pth\n",
            "100%|██████████| 115730790/115730790 [00:01<00:00, 99358261.23it/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created from base_net DenseNet161, Clf H1 with {'out': 102, 'd': 0.2, 'h1': 1024, 'h2': 512}\n",
            "conv0 is unfrozen\n",
            "norm0 is unfrozen\n",
            "relu0 is unfrozen\n",
            "pool0 is unfrozen\n",
            "denseblock1 is unfrozen\n",
            "transition1 is unfrozen\n",
            "denseblock2 is unfrozen\n",
            "transition2 is unfrozen\n",
            "denseblock3 is unfrozen\n",
            "transition3 is unfrozen\n",
            "denseblock4 is unfrozen\n",
            "norm5 is unfrozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXrWb7vbPjFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create workers, \n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "ada = sy.VirtualWorker(hook, 'ada')\n",
        "bob = sy.VirtualWorker(hook, 'bob')\n",
        "cyd = sy.VirtualWorker(hook, 'cyd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sif7E44LLnv4",
        "colab_type": "code",
        "outputId": "6fe1087b-1b76-4453-9690-34544cf799cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create the data loaders, federated PySyft loaders are returned\n",
        "my_trainset, my_validset, _ = create_loaders(model.base_name, isFinal)\n",
        "train_loader = dataset_federate(my_trainset, (ada,bob,cyd))\n",
        "valid_loader = dataset_federate(my_validset, (ada,bob,cyd))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "returning datasets\n",
            "dataset_federate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfPMF2JZ05r",
        "colab_type": "text"
      },
      "source": [
        "### start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P6WWu0nNxny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'objects of ada= {len(ada._objects)}, bob= {len(bob._objects)}, cyd= {len(cyd._objects)}')\n",
        "\n",
        "\n",
        "##### START THE TRAINING #### \n",
        "train_model(model, epochs, isFinal, train_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NToncEuLt-ns",
        "colab_type": "text"
      },
      "source": [
        "### Clear the worker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1G7HggpsFE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ada.clear_objects()\n",
        "bob.clear_objects()\n",
        "cyd.clear_objects()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}