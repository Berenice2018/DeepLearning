{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "PySyft Keystone 2 with Secure Training CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berenice2018/DeepLearning/blob/master/PySyft_Keystone_2_with_Secure_Training_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf9WwGD-uu5Q",
        "colab_type": "text"
      },
      "source": [
        "(notebook copy from Kaggle)\n",
        "\n",
        "**Issues with PySyft: (state of 2019, August 20th)** \n",
        "\n",
        "1.   Unfortunately, PySyft does not support CUDA. https://github.com/OpenMined/PySyft/issues/1893\n",
        "That is why I decided for a simple and small dataset. \n",
        "2.   https://github.com/OpenMined/PySyft/issues/2425\n",
        "3.https://github.com/OpenMined/PySyft/issues/2426 \n",
        "4. \"AttributeError: 'Linear' object has no attribute 'fix_prec'\"\n",
        "https://github.com/OpenMined/PySyft/pull/2364\n",
        "5. Running PySyft code often ends up with an empty AssertionError. It is related to the installation process, because `pip install` could not install succesfully. The current PySyft version does not fit with the current PyTorch, TorchVision versions. \n",
        "6. PySyft currenlty only supports exactly 2 workers, neither more no less. \n",
        "7. We get error messages, when we train with non-linear models, for example while using Convolutional and BatchNorm layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0pO5YODqCl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Data is shared in an encrypted way across multiple end devices via workers\n",
        "```(data.fix_precision().share(ada, bob, crypto_provider= crypto_provdr)```\n",
        "\n",
        "The end device downloads the current model, i.e. the model is shared with the end devices: \n",
        "``` model.fix_precision().share(ada,bob, crypto_provider=crypto_provdr)```\n",
        "\n",
        "\n",
        "The end device improves the model by learning from data on that end device, \n",
        "and then summarizes the changes as a small focused update. \n",
        "\n",
        "It is just this update to the model that is sent to the cloud (secure worker), using encryption, where it is averaged with other updates (coming from end devices) to improve the shared model. \n",
        "\n",
        "All the training data remains on the end devices. The model, the inputs, the model outputs, the weights, etc. will be encrypted as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2SWovhrtGBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "q1DbkeJZxdAq",
        "colab_type": "code",
        "outputId": "a779cdd4-ac7d-426d-bf2d-6cf600f49dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\n",
        "import numpy as np \n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import syft as sy\n",
        "\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 21:48:32.085886 139788239566720 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0827 21:48:32.097706 139788239566720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dbYGqMYoxdAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize plot\n",
        "def plot_loss_acc(n_epochs, train_losses, valid_losses, valid_accuracies):\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(14,6), ncols=2)\n",
        "    ax1.plot(valid_losses, label='Validation loss')\n",
        "    ax1.plot(train_losses, label='Training loss')\n",
        "    ax1.legend(frameon=False)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    #x_ticks = [x for x in range(0,n_epochs,2)]\n",
        "    #plt.xticks(x_ticks)\n",
        "    \n",
        "    ax2.plot(valid_accuracies, label = 'Validation accuracy')\n",
        "    ax2.legend(frameon=False)\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    \n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "hxADDCgNxdAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create workers, \n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "ada = sy.VirtualWorker(hook, 'ada')\n",
        "bob = sy.VirtualWorker(hook, 'bob')\n",
        "\n",
        "crypto_provider = sy.VirtualWorker(hook, 'crypto_provdr') #gives crypto primitives we may need"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WXENc3QoxdAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# load the datasets\n",
        "fulltrainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST('../pytorch/data', download=True, train=False, transform=transform)\n",
        "\n",
        "train_size = int(len(fulltrainset)* 0.8)\n",
        "valid_size = len(fulltrainset) - train_size\n",
        "\n",
        "# split the dataset\n",
        "trainset, validationset = torch.utils.data.random_split(fulltrainset, [train_size, valid_size])\n",
        "trainset = trainset.dataset\n",
        "validationset = validationset.dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEtLzRvywmKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(index_tensor):\n",
        "  #Transform to one hot tensor\n",
        "\n",
        "  onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes\n",
        "  onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
        "  return onehot_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "azt73D0ixdA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
        "n_train_items = 64\n",
        "n_test_items = 64\n",
        "batch_size = 8\n",
        "\n",
        "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
        "        \n",
        "    def secret_share(tensor):\n",
        "        \"\"\"\n",
        "        Transform to fixed precision and secret share a tensor\n",
        "        \"\"\"\n",
        "        return (\n",
        "            tensor\n",
        "            .fix_precision(precision_fractional=precision_fractional)\n",
        "            .share(ada, bob, crypto_provider=crypto_provider)\n",
        "        )\n",
        "    \n",
        "    transformation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
        "        batch_size= batch_size\n",
        "    )\n",
        "    \n",
        "    private_train_loader = [\n",
        "        (secret_share(data), secret_share(one_hot(target)))\n",
        "        for i, (data, target) in enumerate(train_loader)\n",
        "        if i < n_train_items / batch_size\n",
        "    ]\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, download=True, transform=transformation),\n",
        "        batch_size= batch_size\n",
        "    )\n",
        "    \n",
        "    private_test_loader = [\n",
        "        (secret_share(data), secret_share(target.float()))\n",
        "        for i, (data, target) in enumerate(test_loader)\n",
        "        if i < n_test_items / batch_size\n",
        "    ]\n",
        "    \n",
        "    return private_train_loader, private_test_loader\n",
        "    \n",
        "    \n",
        "private_train_loader, private_valid_loader = get_private_data_loaders(\n",
        "    precision_fractional= 3,\n",
        "    workers= [ada, bob],\n",
        "    crypto_provider=crypto_provider\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63epObo5xdA2",
        "colab_type": "text"
      },
      "source": [
        "**Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ll1P35xoxdA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        #self.fc1 = nn.Linear(3*32*32, 1024)\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x # F.log_softmax(x, dim=1) Do not call log_softmax here, it is not supported yet. \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_P1_Ma_xdA4",
        "colab_type": "text"
      },
      "source": [
        "**Functions train, test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jelf7lf7xdA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    '''Takes in a module and initializes all linear layers with weight\n",
        "       values taken from a normal distribution.'''\n",
        "    classname = m.__class__.__name__\n",
        "    # for every Linear layer in a model\n",
        "    if classname.find('Linear') != -1:\n",
        "        n = m.in_features\n",
        "        # m.weight.data shoud be taken from a normal distribution\n",
        "        m.weight.data.normal_(0, 1/np.sqrt(n))\n",
        "        # m.bias.data should be 0\n",
        "        m.bias.data.fill_(0)\n",
        "            \n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    # initialize variables to monitor training and validation loss\n",
        "    train_loss = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader): # now it is a private dataset\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        \n",
        "        #output = F.log_softmax(output)\n",
        "        #loss = F.nll_loss(output, target) # NLLLoss does not work with PySyft encryption\n",
        "        batch_size = output.shape[0]\n",
        "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
        "\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(' \\tTrain. Loss: {:.6f}'.format(current_loss))    \n",
        "\n",
        "        # get the loss per batch and accumulate\n",
        "        train_loss += loss.get().float_precision()\n",
        "        \n",
        "        \n",
        "    # calculate the average loss per epoch\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    #print('Epoch: {} \\tTrain. Loss: {:.6f}'.format(epoch, train_loss))    \n",
        "    return train_loss\n",
        "  \n",
        "  \n",
        "  \n",
        "  # perform the encrypted evaluation. The model weights, the data inputs, the prediction and \n",
        "# the target used for scoring are encrypted!\n",
        "\n",
        "def valid_encrypted(args, model, device, loader):\n",
        "    # initialize variables to monitor training and validation loss\n",
        "    valid_loss = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, target) in enumerate(loader): # <-- now it is a distributed dataset\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "\n",
        "        batch_size = output.shape[0]\n",
        "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
        "\n",
        "\n",
        "        # get the loss per batch and accumulate\n",
        "        valid_loss += loss.get().float_precision()\n",
        "        \n",
        "        \n",
        "    # calculate the average loss per epoch\n",
        "    valid_loss = valid_loss/len(loader)\n",
        "    return valid_loss\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ULhlb5y8xdA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = batch_size\n",
        "        self.epochs = 2\n",
        "        self.lr = 0.001\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "model = Model()\n",
        "model.apply(weights_init_normal)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [3, 10, 14], 0.1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU4-pNJOxdA-",
        "colab_type": "text"
      },
      "source": [
        "**Start the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bYWWeC3mxdA_",
        "colab_type": "code",
        "outputId": "7500d713-663f-4f72-a74b-f9360bf2da7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "\n",
        "valid_losses = []\n",
        "train_losses = []\n",
        "valid_accuracies = []\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model = model.fix_precision().share( *[ada, bob], crypto_provider=crypto_provider)\n",
        "optimizer = optimizer.fix_precision()\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    \n",
        "    # initialize variables to monitor training and validation loss\n",
        "    training_loss = 0.0\n",
        "    training_accuracy = 0.0\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "        \n",
        "    training_loss =  train(args, model, device, private_train_loader, optimizer, epoch)\n",
        "    validation_loss, validation_accuracy = valid_encrypted(args, model, device, private_valid_loader)\n",
        "\n",
        "    #if scheduler is not None:\n",
        "      #scheduler.step(validation_loss) # in case of ReduceOnPlateau \n",
        "\n",
        "    ###### print training/validation statistics \n",
        "    train_losses.append(training_loss)\n",
        "    valid_losses.append(validation_loss)\n",
        "    valid_accuracies.append(validation_accuracy)\n",
        "\n",
        "    #hour, minute, second = get_time()\n",
        "    print('Epoch: {} \\tTrain. Loss: {:.6f} \\tValid. Loss: {:.6f} \\t Accur.: {:.6f}'.format(\n",
        "              epoch,\n",
        "              training_loss,\n",
        "              validation_loss,\n",
        "              validation_accuracy ))\n",
        "\n",
        "    ###### TODO: save the model if validation loss has decreased\n",
        "    if validation_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased by {:.6f}'.format(validation_loss - valid_loss_min))\n",
        "        #torch.save(model.state_dict(), save_path)\n",
        "        valid_loss_min = validation_loss\n",
        "    \n",
        "##### visualize\n",
        "plot_loss_acc(args.epochs, train_losses, valid_losses, valid_accuracies)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9592e05aafc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrypto_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrypto_provider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36mmodule_fix_precision_\u001b[0;34m(nn_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;34m\"\"\"Overloads fix_precision for torch.nn.Module.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_is_missing_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0mcreate_grad_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36mcreate_grad_objects\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                 \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cfLBVlAoxdBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(f'objects of ada= {len(ada._objects)}, bob= {len(bob._objects)}')\n",
        "\n",
        "ada.clear_objects()\n",
        "bob.clear_objects()\n",
        "cyd.clear_objects()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}